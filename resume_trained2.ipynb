{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ananya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfplumber\\__init__.py:11\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfminer\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdftypes\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[1;32mc:\\Users\\ananya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\pdftypes.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mccitt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ccittfaxdecode\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlzw\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lzwdecode\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpsparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LIT\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpsparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PSException\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpsparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PSObject\n",
      "File \u001b[1;32mc:\\Users\\ananya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\psparser.py:22\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Any,\n\u001b[0;32m      9\u001b[0m     BinaryIO,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     Union,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m choplist\n\u001b[0;32m     24\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPSException\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\ananya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pdfminer\\utils.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LTComponent\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m  \u001b[38;5;66;03m# For str encoding detection\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# from sys import maxint as INF doesn't work anymore under Python3, but PDF\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# still uses 32 bits ints\u001b[39;00m\n\u001b[0;32m     35\u001b[0m INF \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m31\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ananya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\charset_normalizer\\__init__.py:24\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_bytes, from_fp, from_path, is_binary\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatch, CharsetMatches\n",
      "File \u001b[1;32mc:\\Users\\ananya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\charset_normalizer\\api.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PathLike\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryIO, List, Optional, Set, Union\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     coherence_ratio,\n\u001b[0;32m      7\u001b[0m     encoding_languages,\n\u001b[0;32m      8\u001b[0m     mb_encoding_languages,\n\u001b[0;32m      9\u001b[0m     merge_coherence_ratios,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IANA_SUPPORTED, TOO_BIG_SEQUENCE, TOO_SMALL_SEQUENCE, TRACE\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1528\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1502\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1601\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import docx\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import win32com.client\n",
    "\n",
    "# Load Hugging Face models for classification and NER\n",
    "def load_classification_pipeline(model_name):\n",
    "    return pipeline(\"zero-shot-classification\", model=model_name)\n",
    "\n",
    "def load_ner_pipeline(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    return pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define model names and initialize pipelines\n",
    "classification_model_name = \"facebook/bart-large-mnli\"  # Replace with your preferred classification model\n",
    "classifier = load_classification_pipeline(classification_model_name)\n",
    "\n",
    "ner_model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"  # Replace with your preferred NER model\n",
    "ner_pipeline = load_ner_pipeline(ner_model_name)\n",
    "\n",
    "# Function to get text from various file formats\n",
    "def input_pdf_text(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        return ''.join([page.extract_text() for page in pdf.pages])\n",
    "\n",
    "def input_docx_text(file_path):\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOCX file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def input_doc_text(file_path):\n",
    "    try:\n",
    "        word = win32com.client.Dispatch(\"Word.Application\")\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOC file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return input_pdf_text(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        return input_docx_text(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return input_doc_text(file_path)\n",
    "    return None\n",
    "\n",
    "# Function to classify text into dynamic categories\n",
    "def classify_text(text, categories):\n",
    "    result = classifier(text, candidate_labels=categories)\n",
    "    return result['labels'][0], result['scores'][0]  # return top label and score\n",
    "\n",
    "# Function to extract named entities based on dynamic labels\n",
    "def extract_entities(text, entity_labels):\n",
    "    entities = ner_pipeline(text)\n",
    "    return [(ent['word'], ent['entity']) for ent in entities if ent['entity'] in entity_labels]\n",
    "\n",
    "# Function to annotate text based on dynamic categories and entities\n",
    "def annotate_text(text, categories, entity_labels):\n",
    "    category, confidence = classify_text(text, categories)\n",
    "    entities = extract_entities(text, entity_labels)\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"confidence\": confidence,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "# Function to dynamically extract skills (can be improved with pre-defined skill lists)\n",
    "def extract_skills(text, skills_model=None):\n",
    "    # Use a TF-IDF Vectorizer to identify potential skills from the text\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.sum(axis=0).A1\n",
    "    skills = [feature_names[i] for i in scores.argsort()[::-1] if scores[i] > 0.1]  # threshold can be adjusted\n",
    "    return skills\n",
    "\n",
    "# Function to parse a single resume\n",
    "def parse_resume(file_path, categories, entity_labels):\n",
    "    text = extract_text(file_path)\n",
    "    if not text:\n",
    "        print(f\"Failed to extract text from {file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Extract relevant information\n",
    "    skills = extract_skills(text, None)  # skills_model can be None if using TF-IDF\n",
    "\n",
    "    # Dynamically annotate the text based on categories and entities\n",
    "    annotations = annotate_text(text, categories, entity_labels)\n",
    "\n",
    "    # Parse results\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": \", \".join(skills) if skills else None,\n",
    "        \"annotations\": annotations\n",
    "    }\n",
    "\n",
    "# Function to load and parse all resumes in a directory\n",
    "def load_resumes(directory, categories, entity_labels):\n",
    "    resumes = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(('.pdf', '.docx', '.doc')):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            parsed_resume = parse_resume(file_path, categories, entity_labels)\n",
    "            if parsed_resume:\n",
    "                resumes.append(parsed_resume)\n",
    "    return resumes\n",
    "\n",
    "# Function to match resumes with job descriptions using TF-IDF and cosine similarity\n",
    "def match_resume_with_jd(resume_text, job_description):\n",
    "    documents = [resume_text, job_description]\n",
    "    tfidf = TfidfVectorizer().fit_transform(documents)\n",
    "    similarity_matrix = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity_matrix[0][0]  # Return the similarity score\n",
    "\n",
    "# Function to segregate files by job descriptions\n",
    "def segregate_files_by_jd(resumes, job_descriptions, base_directory):\n",
    "    if not os.path.exists(base_directory):\n",
    "        os.makedirs(base_directory)\n",
    "\n",
    "    for resume in resumes:\n",
    "        highest_match = 0\n",
    "        best_jd = None\n",
    "        \n",
    "        for jd_title, jd_text in job_descriptions.items():\n",
    "            match_score = match_resume_with_jd(resume['text'], jd_text)\n",
    "            if match_score > highest_match:\n",
    "                highest_match = match_score\n",
    "                best_jd = jd_title\n",
    "\n",
    "        if best_jd:\n",
    "            jd_directory = os.path.join(base_directory, best_jd)\n",
    "            if not os.path.exists(jd_directory):\n",
    "                os.makedirs(jd_directory)\n",
    "\n",
    "            # Sanitize the resume name to avoid illegal characters\n",
    "            resume_filename = f\"resume_{resumes.index(resume)}.txt\"\n",
    "            file_path = os.path.join(jd_directory, resume_filename)\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(resume['text'])\n",
    "                print(f\"Moved resume {resumes.index(resume)} to {jd_directory}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing file {file_path}: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths and job descriptions\n",
    "    directory = \"C:/Users/ananya/Downloads/resumes1\"  # Directory containing resumes\n",
    "    base_directory = \"C:/Users/ananya/Downloads/RESULT_MAX2\"  # Directory to save categorized resumes\n",
    "    job_descriptions = {\n",
    "        \"AI engineer\": \"The AI Engineer designs and implements artificial intelligence models and algorithms...\",\n",
    "        \"Business development executive\": \"The Business Development Executive is responsible for identifying...\",\n",
    "        \"Subject matter expert\": \"The Subject Matter Expert provides expert knowledge and insights...\"\n",
    "    }\n",
    "    \n",
    "    categories = [\"education\", \"skill\", \"experience\", \"interest\", \"personal information\"]\n",
    "    entity_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]  # Adjust according to the NER model used\n",
    "    \n",
    "    resumes = load_resumes(directory, categories, entity_labels)\n",
    "    segregate_files_by_jd(resumes, job_descriptions, base_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
