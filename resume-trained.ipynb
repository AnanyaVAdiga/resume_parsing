{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:/Users/ananya/Downloads/resumes1\\1707727659390_Gudi Madhu Latha-CV.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Aakash_Muthreja_CV1 (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Aakash_Muthreja_CV1.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Ajay _Resume_2024_PM.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\ALOK NATH (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Aswathi p_Mcc (4).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Atul Narayan.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Avkash-Resume.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Brototi Banerjee_Final_CV_Updated (2).docx\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\CURRICULUM VITAE.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\cvnd24 (3).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\CV_2023070509122292.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\CV_Resume.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\CV_Sudheer Kumar Puvvula_Latest.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\DHANYA 1 (1).docx\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Dhruthi R.docx\n",
      "Failed to extract text from C:/Users/ananya/Downloads/resumes1\\Dhruthi R.docx\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Digital Marketing Resume Himanshu (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\DOC-20230803-WA0013 (7).docx\n",
      "Error processing DOCX file C:/Users/ananya/Downloads/resumes1\\DOC-20230803-WA0013 (7).docx: Bad CRC-32 for file 'word/media/image1.png'\n",
      "Failed to extract text from C:/Users/ananya/Downloads/resumes1\\DOC-20230803-WA0013 (7).docx\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Gopika_Gopan_Resume_2024.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Hansa cv.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Hari Kunjeti -Resume (1)-converted (1) (2) (1).docx\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Himanshu Resume.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Kumaraswamy-Designer-070524.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\ManeshSreedharResume (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\MANIKANTHA POOJARI .pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Michelraj J_Resume (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Mohamed ashik (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\MY CV Soubhik (2).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\MY CV Yashaswini.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\N Gautham Bhat CV-2.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\pdf&rendition=1 (1).pdf\n",
      "Failed to extract text from C:/Users/ananya/Downloads/resumes1\\pdf&rendition=1 (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Priyam Ray Updated Resume - New (1) (2).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Ragu (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\reaume self.docx\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\RESUME NAZHATH FATHIMA_101505 (1).doc\n",
      "Error processing DOC file C:/Users/ananya/Downloads/resumes1\\RESUME NAZHATH FATHIMA_101505 (1).doc: (-2147352567, 'Exception occurred.', (0, 'Microsoft Word', \"Sorry, we couldn't find your file. Was it moved, renamed, or deleted?\\r (C:\\\\//Users/ananya/Downloads/resumes1/...)\", 'wdmain11.chm', 24654, -2146823114), None)\n",
      "Failed to extract text from C:/Users/ananya/Downloads/resumes1\\RESUME NAZHATH FATHIMA_101505 (1).doc\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Resume.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\resume2 (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Sagar M - Work resume.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Share Praful wasekar 2021 update resume-1 (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Srresume 1.pdf\n",
      "Failed to extract text from C:/Users/ananya/Downloads/resumes1\\Srresume 1.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Subhankar Chakraborty Resume.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\SubhankarMohantyResume .pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\SushanthB Resume 1.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\TANISHA GHOSH..docx.pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Thejeswini_Resume_6_Yrs_Exp (1).docx\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Vineesh-1 (1).pdf\n",
      "Processing file: C:/Users/ananya/Downloads/resumes1\\Yashaswini Resume.pdf\n",
      "Moved resume 0 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 1 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 1 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 3 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 4 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 5 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 6 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 7 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 8 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 9 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 10 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 11 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 12 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 13 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 14 to C:/Users/ananya/Downloads/RESULT_MAX\\Business development executive\n",
      "Moved resume 15 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 16 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 17 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 18 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 19 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 20 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 21 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 22 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 23 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 24 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 25 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 26 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 27 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 28 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 29 to C:/Users/ananya/Downloads/RESULT_MAX\\Subject matter expert\n",
      "Moved resume 31 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 32 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 33 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 34 to C:/Users/ananya/Downloads/RESULT_MAX\\Business development executive\n",
      "Moved resume 35 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 36 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 37 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 38 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 39 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 40 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n",
      "Moved resume 41 to C:/Users/ananya/Downloads/RESULT_MAX\\AI engineer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import docx\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import win32com.client\n",
    "\n",
    "# Load Hugging Face models for classification and NER\n",
    "def load_classification_pipeline(model_name):\n",
    "    return pipeline(\"zero-shot-classification\", model=model_name)\n",
    "\n",
    "def load_ner_pipeline(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    return pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define model names and initialize pipelines\n",
    "classification_model_name = \"distilbert-base-uncased\"  # Replace with your preferred classification model\n",
    "classifier = load_classification_pipeline(classification_model_name)\n",
    "\n",
    "ner_model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"  # Replace with your preferred NER model\n",
    "ner_pipeline = load_ner_pipeline(ner_model_name)\n",
    "\n",
    "# Function to get text from various file formats\n",
    "def input_pdf_text(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        return ''.join([page.extract_text() for page in pdf.pages])\n",
    "\n",
    "def input_docx_text(file_path):\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOCX file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def input_doc_text(file_path):\n",
    "    try:\n",
    "        word = win32com.client.Dispatch(\"Word.Application\")\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOC file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return input_pdf_text(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        return input_docx_text(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return input_doc_text(file_path)\n",
    "    return None\n",
    "\n",
    "# Function to classify text into dynamic categories\n",
    "def classify_text(text, categories):\n",
    "    result = classifier(text, candidate_labels=categories)\n",
    "    return result['labels'][0], result['scores'][0]  # return top label and score\n",
    "\n",
    "# Function to extract named entities based on dynamic labels\n",
    "def extract_entities(text, entity_labels):\n",
    "    entities = ner_pipeline(text)\n",
    "    return [(ent['word'], ent['entity']) for ent in entities if ent['entity'] in entity_labels]\n",
    "\n",
    "# Function to annotate text based on dynamic categories and entities\n",
    "def annotate_text(text, categories, entity_labels):\n",
    "    category, confidence = classify_text(text, categories)\n",
    "    entities = extract_entities(text, entity_labels)\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"confidence\": confidence,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "# Function to dynamically extract skills (can be improved with pre-defined skill lists)\n",
    "def extract_skills(text, skills_model=None):\n",
    "    # Use a TF-IDF Vectorizer to identify potential skills from the text\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.sum(axis=0).A1\n",
    "    skills = [feature_names[i] for i in scores.argsort()[::-1] if scores[i] > 0.1]  # threshold can be adjusted\n",
    "    return skills\n",
    "\n",
    "# Function to parse a single resume\n",
    "def parse_resume(file_path, categories, entity_labels):\n",
    "    text = extract_text(file_path)\n",
    "    if not text:\n",
    "        print(f\"Failed to extract text from {file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Extract relevant information\n",
    "    skills = extract_skills(text, None)  # skills_model can be None if using TF-IDF\n",
    "\n",
    "    # Dynamically annotate the text based on categories and entities\n",
    "    annotations = annotate_text(text, categories, entity_labels)\n",
    "\n",
    "    # Parse results\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"skills\": \", \".join(skills) if skills else None,\n",
    "        \"annotations\": annotations\n",
    "    }\n",
    "\n",
    "# Function to load and parse all resumes in a directory\n",
    "def load_resumes(directory, categories, entity_labels):\n",
    "    resumes = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(('.pdf', '.docx', '.doc')):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            parsed_resume = parse_resume(file_path, categories, entity_labels)\n",
    "            if parsed_resume:\n",
    "                resumes.append(parsed_resume)\n",
    "    return resumes\n",
    "\n",
    "# Function to match resumes with job descriptions using TF-IDF and cosine similarity\n",
    "def match_resume_with_jd(resume_text, job_description):\n",
    "    documents = [resume_text, job_description]\n",
    "    tfidf = TfidfVectorizer().fit_transform(documents)\n",
    "    similarity_matrix = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity_matrix[0][0]  # Return the similarity score\n",
    "\n",
    "# Function to segregate files by job descriptions\n",
    "def segregate_files_by_jd(resumes, job_descriptions, base_directory):\n",
    "    if not os.path.exists(base_directory):\n",
    "        os.makedirs(base_directory)\n",
    "\n",
    "    for resume in resumes:\n",
    "        highest_match = 0\n",
    "        best_jd = None\n",
    "        \n",
    "        for jd_title, jd_text in job_descriptions.items():\n",
    "            match_score = match_resume_with_jd(resume['text'], jd_text)\n",
    "            if match_score > highest_match:\n",
    "                highest_match = match_score\n",
    "                best_jd = jd_title\n",
    "\n",
    "        if best_jd:\n",
    "            jd_directory = os.path.join(base_directory, best_jd)\n",
    "            if not os.path.exists(jd_directory):\n",
    "                os.makedirs(jd_directory)\n",
    "\n",
    "            # Sanitize the resume name to avoid illegal characters\n",
    "            resume_filename = f\"resume_{resumes.index(resume)}.txt\"\n",
    "            file_path = os.path.join(jd_directory, resume_filename)\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(resume['text'])\n",
    "                print(f\"Moved resume {resumes.index(resume)} to {jd_directory}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing file {file_path}: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths and job descriptions\n",
    "    directory = \"C:/Users/ananya/Downloads/resumes1\"  # Directory containing resumes\n",
    "    base_directory = \"C:/Users/ananya/Downloads/RESULT_MAX\"  # Directory to save categorized resumes\n",
    "    job_descriptions = {\n",
    "        \"AI engineer\": \"The AI Engineer designs and implements artificial intelligence models and algorithms...\",\n",
    "        \"Business development executive\": \"The Business Development Executive is responsible for identifying...\",\n",
    "        \"Subject matter expert\": \"The Subject Matter Expert provides expert knowledge and insights...\"\n",
    "    }\n",
    "    \n",
    "    categories = [\"education\", \"skill\", \"experience\", \"interest\", \"personal information\"]\n",
    "    entity_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]  # Adjust according to the NER model used\n",
    "    \n",
    "    resumes = load_resumes(directory, categories, entity_labels)\n",
    "    segregate_files_by_jd(resumes, job_descriptions, base_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
